---
title: "Simple streaming models in the cloud, but with Python via Pulumi"
author: "Serguei Ossokine"

format:
    html:
        theme: cosmo
        code-tools: true
        page-layout: full
        number-offset: 1
        code-summary: "Show the code"
        number-sections: true

date: last-modified
from: markdown+emoji

filters:
  - d2
d2:
  layout: "dagre"
  theme: "NeutralDefault"
---
# Introduction

In  previous posts ([1](https://sergeiossokine.github.io/posts/streaming_deployment/streaming_example.html),[2](https://sergeiossokine.github.io/posts/streaming_deployment_part2/stream_example_contd.html)) I discussed deploying a simple streaming model to AWS, using AWS cli, CloudFormation and Terraform. In this blog, I re-deploy the same model, but this time using another fairly different approach to IaC by using [`Pulumi`](https://www.pulumi.com/). `Pulumi` is an open source IaC tool that allows one to use one of several programming languages (TypeScript, Python, Go, C#, Java) as well as YAML. Being very fond of Python, I of course will use that.

# Using Pulumi for IaC

::: {.callout-note}
The configuration files are in [this repo](https://github.com/SergeiOssokine/mlops-zoomcamp-misc/tree/main/streaming-deployment/model_deployment_pulumi).
If you have not done the deployments in the [previous post](https://sergeiossokine.github.io/posts/streaming_deployment/streaming_example.html), you will need to build the docker image and push it to the ECR before you can proceed - see [this part](https://sergeiossokine.github.io/posts/streaming_deployment/streaming_example.html#put-everything-together).
:::

## Preliminaries
As usual, before getting to the actual IaC we need to train the model and store it in a S3 bucket.
As before we do:
```bash
uuid=$(uuidgen)
export BUCKET_NAME=nyc-taxi-example-${uuid}
export REGION=us-east-1
export INPUT_STREAM="ride_events"
export OUTPUT_STREAM="ride_predictions"
```

```bash
aws s3api create-bucket --bucket ${BUCKET_NAME} --region ${REGION}
```

Now start the `mlflow` server (no changes here from the last post):
```bash
cd model-training
mlflow server --default-artifact-root s3://${BUCKET_NAME} --backend-store-uri sqlite:///mlflow.db --port 5012
```

Finally train the model:
```bash
python train_model.py
```


We also need to store the run ID of the model. You can find it through the `MLFlow` UI or API. Then do

```bash
export RUN_ID=RUNIDHERE
```

Finally we need the uri of the Docker image we pushed to the ECR repo:
```bash
export REMOTE_IMAGE=URIHERE
```


## Using Pulumi for IaC
::: {.callout-important}
Some/all services used in this section cost money. Always ensure you are aware of the associated costs.
:::

First things first, we need to set a passphrase to protect any secrets. By default this is done with a passphrase you can do this with

```bash
export  PULUMI_CONFIG_PASSPHRASE="YOURPASSPHRASE"
```
Now we initialize a new stack. Change to the `model_deployment_pulumi` directory and run

```bash
pulumi stack init dev
```

This will create a file `Pulumi.dev.yaml` that will be essentially empty. It will also initialize a lot of internal details (such as the stack state) in the default directory, which on linux is `~/.pulumi/`. In principle, one should never interact with that directory directly.

Now we update this file with all the necessary config settings:

```bash
pulumi config set-all --plaintext aws-native:region=${REGION} --plaintext model_bucket=${BUCKET_NAME} --plaintext run_id=${RUN_ID} --plaintext image_uri=${REMOTE_IMAGE} --plaintext input_stream_name=${INPUT_STREAM} --plaintext output_stream_name=${OUTPUT_STREAM}
```

That should be it for congiruation.

The code that actually provisions the resources is (appropriately) inside `__main__.py`. Note that at the time of writing:

- There are 2 different ways to use AWS resources with `pulumi`: `aws` and `aws_native`. The former is stable and supported but the latter is planned to be the replacement, so we use that instead.
- There is an error in the `aws_native` documentation about the way that inline policies should be defined. See [here](https://github.com/pulumi/pulumi-aws-native/issues/1616)

The code is pretty self-explanatory and mimics quite closely the way we have seen resouces provisioned with `terraform` and `CloudFormation`.


You can now preview the resources that will be deployed by running

```bash
pulumi preview
```
You should see somthing like

```
Previewing update (dev):
@ previewing update....

 +  pulumi:pulumi:Stack streaming_model-dev create
@ previewing update.....
 +  aws-native:kinesis:Stream ride_predictions create
 +  aws-native:kinesis:Stream ride_events create
 +  aws-native:iam:Role LambdaKinesisIAMRole create
 +  aws-native:lambda:Function predict create
 +  aws-native:lambda:EventSourceMapping output_mapping create
 +  pulumi:pulumi:Stack streaming_model-dev create
Outputs:
    input_stream_arn : output<string>
    output_stream_arn: output<string>

Resources:
    + 6 to create
```



Now we are ready to deploy the infrastructure. Run

```bash
pulumi up
```
then choose, "yes" and hit enter. You should see something like:

```
Previewing update (dev):
     Type                                     Name                  Plan
 +   pulumi:pulumi:Stack                      streaming_model-dev   create
 +   ├─ aws-native:kinesis:Stream             ride_predictions      create
 +   ├─ aws-native:kinesis:Stream             ride_events           create
 +   ├─ aws-native:iam:Role                   LambdaKinesisIAMRole  create
 +   ├─ aws-native:lambda:Function            predict               create
 +   └─ aws-native:lambda:EventSourceMapping  output_mapping        create

Outputs:
    input_stream_arn : output<string>
    output_stream_arn: output<string>

Resources:
    + 6 to create

Do you want to perform this update? yes
Updating (dev):
     Type                                     Name                  Status
 +   pulumi:pulumi:Stack                      streaming_model-dev   created (74s)
 +   ├─ aws-native:kinesis:Stream             ride_predictions      created (7s)
 +   ├─ aws-native:kinesis:Stream             ride_events           created (8s)
 +   ├─ aws-native:iam:Role                   LambdaKinesisIAMRole  created (24s)
 +   ├─ aws-native:lambda:Function            predict               created (37s)
 +   └─ aws-native:lambda:EventSourceMapping  output_mapping        created (2s)

Outputs:
    input_stream_arn : "arn:aws:kinesis:us-east-1:XXXXXXXXXXXX:stream/ride_events"
    output_stream_arn: "arn:aws:kinesis:us-east-1:XXXXXXXXXXXX:stream/ride_predictions"

Resources:
    + 6 created

Duration: 1m15s
```

::: {.callout-note}
If you get an error like this:

```bash
error: --yes or --skip-preview must be passed in to proceed when running in non-interactive mode
```
then instead execute `PULUMI_DISABLE_CI_DETECTION=true pulumi up`. See the discussion [here](https://github.com/pulumi/pulumi/issues/4503)
:::


As always we can test everything is working right by putting an event into the input stream as follows

```bash
aws kinesis put-record     --cli-binary-format raw-in-base64-out --stream-name ${INPUT_STREAM}     --partition-key 999     --data '{
        "ride": {
            "PULocationID": 30,
            "DOLocationID": 105,
            "trip_distance": 1.66
        },
        "ride_id": 999
    }'
```

You can either then look in the AWS management console or wait a couple of minutes and run


which should result in:

```json
{
  "model": "ride_duration_prediction_model",
  "version": "123",
  "prediction": {
    "ride_duration": 10.830198691258547,
    "ride_id": 999
  }
}
```

To clean up, simply run

```bash
pulumi down
```

## Automatic conversion from `terraform`
`pulumi` supports migration from several different other IaC tools, including `terraform`, as described [here](https://www.pulumi.com/docs/using-pulumi/adopting-pulumi/migrating-to-pulumi/#conversion). For `terraform` there is a built-in tool to generate `pulumi` code. Since we already have a `terraform` implementation, it's interesting to see what kind of code would be generated. To do so, navigate to `model_deployment_tf` and run [^1]

```bash
pulumi convert --from terraform --language python
```
You will also need to initialize the stack with

```bash
pulumi stack init dev
```

The tool is clever enough to recognize that the `terraform` implementation used modules for AWS Kinesis and AWS Lambda and creates 2 files `kinesis.py` and `lambda.py` which implement corresponding Python classes. It also preserves the correct comments from terraform and even adds trhe descriptions of the `terraform` variables as comments prior to getting them from the config, e.g. [here]().

In my conversion, the code produced was not syntactically correct, but fortunately the errors were mostly trivial, for example, running `pulumi preview` on the raw generated code gave errors like:

``` bash
File "mlops-zoomcamp-misc/streaming-deployment/model_deployment_tf/__main__.py", line 3
    from lambda import Lambda
            ^^^^^^
SyntaxError: invalid syntax
```
or (which happens several times)

```bash
File "mlops-zoomcamp-misc/streaming-deployment/model_deployment_tf/__main__.py", line 3, in <module>
from lambda_ import Lambda
File "mlops-zoomcamp-misc/streaming-deployment/model_deployment_tf/lambda_.py", line 21
name=f"iam_{args["lambdaFunctionName"]}",
                    ^^^^^^^^^^^^^^^^^^
SyntaxError: f-string: unmatched '['
```
Fortunately, in this particular case they are all trivial to fix: in the first case, simply rename `lambda.py` to `lambda_.py` and change the import statement, in the latter, correct the quotes so that the ones used inside the {} are different from the outside[^2].
The other issue arises with default values of variables set inside a module, in particular in the `kinesis` module we had a variable like this:

```terraform

variable "shard_level_metrics" {
  type        = list(string)
  description = "shard_level_metrics"
  default = [
    "IncomingBytes",
    "OutgoingBytes",
    "OutgoingRecords",
    "ReadProvisionedThroughputExceeded",
    "WriteProvisionedThroughputExceeded",
    "IncomingRecords",
    "IteratorAgeMilliseconds",
  ]
}
```

While the script correctly recognizes that `shardLeveMetrics` should be set in `kinesis.py`, it doesn't either set the default in the `Kinesis` class inside `kinesis.py` nor pass `shardLevelMetrics` to the class constructor inside `__main__.py`. To fix this, we simply pass this argument inside `__main__.py`,as shown [here](https://github.com/SergeiOssokine/mlops-zoomcamp-misc/blob/main/streaming-deployment/model_deployment_pulumi_autogen/__main__.py#L34).  I also had to fix a whole bunch of things and in the process encountered the concepts out [`Outputs`](https://www.pulumi.com/docs/concepts/inputs-outputs/apply/#accessing-single-output-values) in `pulumi` which were a bit awkward. The final version of the code is [here](https://github.com/SergeiOssokine/mlops-zoomcamp-misc/tree/main/streaming-deployment/model_deployment_pulumi_autogen)


Finally, we still need to add all the correct config variables. You can do this by running:


```bash
pulumi config set-all --path --plaintext aws-native:region=${REGION} --plaintext modelBucket=${BUCKET_NAME} --plaintext runId=${RUN_ID} --plaintext image_uri=${REMOTE_IMAGE} --plaintext sourceStreamName=${INPUT_STREAM} --plaintext outputStreamName=${OUTPUT_STREAM} --plaintext lambdaFunctionName="predict-lambda" --plaintext  "shardLevelMetrics[0]"=IncomingBytes  --plaintext "shardLevelMetrics[1]"=OutgoingBytes
```

Doing `pulumi preview` should now show that all the resources we want will be deployed which you can do with `pulumi up`.

For our simple stack here this certainly wasn't too painful, but I was happy to learn more about `pulumi` by doing it from scratch.

## Local testing with `localstack`
Just as one can use `localstack` to test `terraform` code, one can do the same with `pulumi` as well. All one has to do is to install the handy `pulumilocal` [script](https://docs.localstack.cloud/user-guide/integrations/pulumi/) and proceed as before.

[^1]: Note that at the time of writing, this will generate python code using the `aws` module, not `aws_native`
[^2]: I think this particular bug can be avoided simply by always using triple quotes, f""" for the generated f-string expression